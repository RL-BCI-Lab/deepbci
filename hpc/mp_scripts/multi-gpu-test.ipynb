{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "import time \n",
    "import numpy as np\n",
    "import os\n",
    "import multi_gpu as mg\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Time: 0.000184 Tasks Left: 4\n",
      "Time: 1.001857 Tasks Left: 4\n",
      "Time: 2.012834 Tasks Left: 4\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n",
      "Time: 3.013831 Tasks Left: 4\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n",
      "Time: 4.019897 Tasks Left: 4\n",
      "Time: 5.023826 Tasks Left: 4\n",
      "130411  returning:  2.2138309478759766\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n",
      "Time: 6.028813 Tasks Left: 3\n",
      "Time: 7.036811 Tasks Left: 3\n",
      "Time: 8.045818 Tasks Left: 3\n",
      "Time: 9.046906 Tasks Left: 3\n",
      "130412  returning:  5.2393951416015625\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n",
      "Time: 10.055813 Tasks Left: 2\n",
      "Time: 11.056816 Tasks Left: 2\n",
      "Time: 12.057810 Tasks Left: 2\n",
      "Time: 13.058811 Tasks Left: 2\n",
      "Time: 14.059810 Tasks Left: 2\n",
      "Time: 15.068835 Tasks Left: 2\n",
      "130411  returning:  9.803203821182251\n",
      "Time: 16.070813 Tasks Left: 1\n",
      "Time: 17.072811 Tasks Left: 1\n",
      "Time: 18.074809 Tasks Left: 1\n",
      "Time: 19.076808 Tasks Left: 1\n",
      "Time: 20.078808 Tasks Left: 1\n",
      "Time: 21.088808 Tasks Left: 1\n",
      "130412  returning:  12.513060808181763\n",
      "[True, True, True, True]\n",
      "Results: [2.2138309478759766, 5.2393951416015625, 9.803203821182251, 12.513060808181763]\n"
     ]
    }
   ],
   "source": [
    "shape = (10000, 500)\n",
    "steps = 10\n",
    "NUM_PROC = mp.cpu_count()\n",
    "NUM_GPUS = 2\n",
    "manager = mp.Manager()\n",
    "gpu_queue = manager.Queue(maxsize=NUM_GPUS)\n",
    "for i in range(NUM_GPUS):\n",
    "    gpu_queue.put(i)\n",
    "\n",
    "default_kwargs = dict(func=mg.train, gpu_queue=gpu_queue)\n",
    "kwargs = [\n",
    "    dict(**default_kwargs, x=shape, steps=10),\n",
    "    dict(**default_kwargs, x=shape, steps=30),\n",
    "    dict(**default_kwargs, x=shape, steps=60),\n",
    "    dict(**default_kwargs, x=shape, steps=80)\n",
    "]\n",
    "print(NUM_GPUS)\n",
    "results = []\n",
    "log_results = lambda result: results.append(result)\n",
    "    \n",
    "pool = mp.Pool(NUM_GPUS)\n",
    "jobs = []\n",
    "for kws in kwargs:\n",
    "    jobs.append(pool.apply_async(mg.multi_gpu, kwds=kws, callback=log_results))\n",
    "pool.close()\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    check_jobs = [j.ready() for j in jobs]\n",
    "    if not all(check_jobs):\n",
    "        jobs_left = len(jobs) - sum(check_jobs)\n",
    "        print('Time: {:4f} Tasks Left: {}'.format(time.time() - start, jobs_left))\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "pool.join()\n",
    "print([j.successful() for j in jobs])\n",
    "print(\"Results: {}\".format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.000176 Tasks Left: 4\n",
      "Time: 1.008430 Tasks Left: 4\n",
      "Time: 2.019429 Tasks Left: 4\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')LogicalDevice(name='/device:GPU:0', device_type='GPU')\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n",
      "\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n",
      "Time: 3.021102 Tasks Left: 4\n",
      "Time: 4.022437 Tasks Left: 4\n",
      "Time: 5.023413 Tasks Left: 4\n",
      "Time: 6.030511 Tasks Left: 4\n",
      "Time: 7.031408 Tasks Left: 4\n",
      "Time: 8.041418 Tasks Left: 4\n",
      "124833  returning:  4.88648533821106\n",
      "Time: 9.044412 Tasks Left: 4\n",
      "Time: 10.055404 Tasks Left: 3\n",
      "Time: 11.058622 Tasks Left: 3\n",
      "Time: 12.059404 Tasks Left: 3\n",
      "Time: 13.060412 Tasks Left: 3\n",
      "124834  returning:  9.964785099029541\n",
      "Time: 14.061413 Tasks Left: 3\n",
      "Time: 15.062406 Tasks Left: 2\n",
      "124835  returning:  11.879923343658447\n",
      "Time: 16.066963 Tasks Left: 1\n",
      "Time: 17.076611 Tasks Left: 1\n",
      "Time: 18.086607 Tasks Left: 1\n",
      "Time: 19.096605 Tasks Left: 1\n",
      "Time: 20.105412 Tasks Left: 1\n",
      "Time: 21.115410 Tasks Left: 1\n",
      "Time: 22.117419 Tasks Left: 1\n",
      "124836  returning:  16.885507106781006\n",
      "[True, True, True, True]\n",
      "Results: [4.88648533821106, 9.964785099029541, 11.879923343658447, 16.885507106781006]\n"
     ]
    }
   ],
   "source": [
    "shape = (10000, 500)\n",
    "steps = 10\n",
    "NUM_PROC = mp.cpu_count()\n",
    "NUM_GPUS = 2\n",
    "manager = mp.Manager()\n",
    "gpu_queue = manager.Queue(maxsize=NUM_GPUS)\n",
    "for i in range(NUM_GPUS):\n",
    "    gpu_queue.put(i)\n",
    "\n",
    "default_kwargs = dict(func=mg.train)\n",
    "kwargs = [\n",
    "    dict(**default_kwargs, gpu=0, x=shape, steps=10),\n",
    "    dict(**default_kwargs, gpu=1, x=shape, steps=30),\n",
    "    dict(**default_kwargs, gpu=0, x=shape, steps=60),\n",
    "    dict(**default_kwargs, gpu=1, x=shape, steps=80)\n",
    "]\n",
    "\n",
    "results = []\n",
    "def log_results(result):\n",
    "    results.append(result)\n",
    "    \n",
    "pool = mp.Pool()\n",
    "jobs = []\n",
    "for kws in kwargs:\n",
    "    jobs.append(pool.apply_async(mg.multi_gpu2, kwds=kws, callback=log_results))\n",
    "pool.close()\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    check_jobs = [j.ready() for j in jobs]\n",
    "    if not all(check_jobs):\n",
    "        jobs_left = len(jobs) - sum(check_jobs)\n",
    "        print('Time: {:4f} Tasks Left: {}'.format(time.time() - start, jobs_left))\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "pool.join()\n",
    "print([j.successful() for j in jobs])\n",
    "print(\"Results: {}\".format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def measure(x, steps, gpu):\n",
    "#     import tensorflow as tf\n",
    "#     physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "#     tf.config.experimental.set_visible_devices(physical_gpus[gpu], 'GPU')\n",
    "#     tf.config.experimental.set_memory_growth(physical_gpus[gpu], True)\n",
    "\n",
    "#     x = tf.random.normal(x)\n",
    "#     x = x @ tf.transpose(x)\n",
    "#     start = time.time()\n",
    "#     for i in range(steps):\n",
    "#         x = x @ tf.transpose(x)\n",
    "#     _ = x.numpy()\n",
    "#     end = time.time()\n",
    "#     print(os.getpid(), ' returning: ', end - start)\n",
    "#     return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'multi_gpu' has no attribute 'measure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-22a4dd7444c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'file{}.xyz'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'multi_gpu' has no attribute 'measure'"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, current_process, Queue\n",
    "\n",
    "NUM_GPUS = 4\n",
    "PROC_PER_GPU = 2    \n",
    "\n",
    "queue = Queue()\n",
    "\n",
    "def foo(filename):\n",
    "    gpu_id = queue.get()\n",
    "    try:\n",
    "        # run processing on GPU <gpu_id>\n",
    "        ident = current_process().ident\n",
    "        print('{}: starting process on GPU {}'.format(ident, gpu_id))\n",
    "        # ... process filename\n",
    "        print('{}: finished'.format(ident))\n",
    "    finally:\n",
    "        queue.put(gpu_id)\n",
    "\n",
    "# initialize the queue with the GPU ids\n",
    "for gpu_ids in range(NUM_GPUS):\n",
    "    for _ in range(PROC_PER_GPU):\n",
    "        queue.put(gpu_ids)\n",
    "\n",
    "pool = Pool(processes=PROC_PER_GPU * NUM_GPUS)\n",
    "files = ['file{}.xyz'.format(x) for x in range(1000)]\n",
    "with mp.Pool(2) as pool:\n",
    "    results = pool.starmap_async(mg.measure, args)\n",
    "    print(results.get())\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_trn, y_trn), (X_tst, y_tst) = mnist.load_data()\n",
    "X_trn, X_tst = X_trn / 255.0, X_tst / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, X_trn, y_trn, X_tst, y_tst, model, optim, loss, metrics=['accuracy']):\n",
    "        self.trn_ds = tf.data.Dataset.from_tensor_slices((X_trn.astype(np.float32), \n",
    "                                                           y_trn.astype(np.float32))).batch(128)\n",
    "        \n",
    "        self.tst_ds = tf.data.Dataset.from_tensor_slices((X_tst.astype(np.float32), \n",
    "                                                            y_tst.astype(np.float32))).batch(128)\n",
    "        self.model = model.compile(optimizer=optim, loss=loss, metrics=metrics)      \n",
    "\n",
    "\n",
    "    def fiteval(self, epochs=1):\n",
    "        hist = self.model.fit(self.trn_ds, epochs)\n",
    "\n",
    "        # Clear tf/keras graph state\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        tst_loss, tst_acc = model.evaluate(self.tst_ds, verbose=2)\n",
    "\n",
    "        # Clear tf/keras graph state\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        return tst_loss, tst_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fiteval_multi_gpu(cfgs, n_gpus):\n",
    "    \"\"\" Runs multiple basic training and then testing experiments \"\"\"\n",
    "    jobs = []\n",
    "    results = []\n",
    "    log_results = lambda result: results.append(result)\n",
    "\n",
    "    # Build available queue which contains GPUs which are aviable for use\n",
    "    manager = mp.Manager()\n",
    "    avail_gpus = manager.Queue(maxsize=n_gpus)\n",
    "    for i in range(n_gpus):\n",
    "        avail_gpus.put(i)\n",
    "\n",
    "    pool = mp.Pool(n_gpus)\n",
    "\n",
    "    for cfg in cfgs:\n",
    "        (cfg_name, kwargs), = cfg.items()\n",
    "\n",
    "        # Build trainer object\n",
    "        trainer_kws = kwargs.pop('trainer')\n",
    "        trainer = Trainer(**trainer_kws)\n",
    "\n",
    "        # Add additional kwargs\n",
    "        kwargs['gpu_avail'] = avail_gpus\n",
    "        kwargs['func'] = trainer.fiteval\n",
    "        set_trace()\n",
    "        # Initialize processes\n",
    "        jobs.append(pool.apply_async(tf_multi_gpu, \n",
    "                                     kwds=kwargs, \n",
    "                                     callback=log_results))\n",
    "\n",
    "    job_status(jobs)\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"Successful Jobs: {} \".format([j.successful() for j in jobs]))\n",
    "   \n",
    "    \n",
    "def tf_multi_gpu(func, avail_gpus, *args, **kwargs):\n",
    "\n",
    "    # Wrap in try-catch so errors will actually get printed\n",
    "    try:\n",
    "        # Get gpu for this process\n",
    "        gpu = avail_gpus.get()\n",
    "        \n",
    "        # Set GPU visible device and set memory growth\n",
    "        physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "        tf.config.experimental.set_visible_devices(physical_gpus[gpu], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(physical_gpus[gpu], True)\n",
    "    \n",
    "        # Run function given selected gpu\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        with tf.device(logical_gpus[0]):\n",
    "            results = func(*args, **kwargs)\n",
    "            \n",
    "        # Add gpu back to being available \n",
    "        avail_gpus.put(gpu)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(e)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def job_status(jobs):\n",
    "    import psutil\n",
    "    import time\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        check_jobs = [j.ready() for j in jobs]\n",
    "        if not all(check_jobs):\n",
    "            jobs_left = len(jobs) - sum(check_jobs)\n",
    "            print('Time: {:4f} Tasks Left: {} Memory: {}'.format(time.time() - start, \n",
    "                                                                jobs_left,\n",
    "                                                                psutil.virtual_memory()[2]))\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-8-1d0289f30214>\u001b[0m(27)\u001b[0;36mrun_fiteval_multi_gpu\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     25 \u001b[0;31m        \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m        \u001b[0;31m# Initialize processes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m        jobs.append(pool.apply_async(tf_multi_gpu, \n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m                                     \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m                                     callback=log_results))\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "> \u001b[0;32m<ipython-input-8-1d0289f30214>\u001b[0m(25)\u001b[0;36mrun_fiteval_multi_gpu\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     23 \u001b[0;31m        \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gpu_avail'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavail_gpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m        \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'func'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiteval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 25 \u001b[0;31m        \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m        \u001b[0;31m# Initialize processes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m        jobs.append(pool.apply_async(tf_multi_gpu, \n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "Successful Jobs: [False, False] \n"
     ]
    }
   ],
   "source": [
    "fc = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(64, activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cnn = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(), \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "cfgs=[\n",
    "    {'exp1': {\n",
    "        'trainer':{\n",
    "            'X_trn': X_trn,\n",
    "            'y_trn': y_trn,\n",
    "            'X_tst': X_tst,\n",
    "            'y_tst': y_tst,\n",
    "            'optim': getattr(tf.keras.optimizers, 'Adam')(.01),\n",
    "            'loss': getattr(tf.keras.losses, 'sparse_categorical_crossentropy'),\n",
    "            'model': fc\n",
    "        },\n",
    "        'epochs': 3\n",
    "     \n",
    "    }},\n",
    "    {'exp2': {\n",
    "        'trainer':{\n",
    "            'X_trn': X_trn,\n",
    "            'y_trn': y_trn,\n",
    "            'X_tst': X_tst,\n",
    "            'y_tst': y_tst,\n",
    "            'optim': tf.keras.optimizers.Adam(.01),\n",
    "            'loss': tf.keras.losses.sparse_categorical_crossentropy,\n",
    "            'model': cnn\n",
    "        },\n",
    "        'epochs': 3\n",
    "    }}\n",
    "]\n",
    "\n",
    "\n",
    "run_fiteval_multi_gpu(cfgs, n_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
